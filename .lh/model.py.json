{
    "sourceFile": "model.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 18,
            "patches": [
                {
                    "date": 1647309532880,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1647309539001,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,15 +2,14 @@\n from surprise import Dataset, Reader\n from typing import List, Tuple, Dict\n import pandas as pd\n import pickle\n-from constants.model_interface import MovieRecommender\n from src.data_processing.create_data import create_dataset_from_file_with_mapping\n from src.data_processing.data_processing_utils import create_movie_id_mappings_from_file\n import time\n \n \n-class Surprise_Algo(MovieRecommender):\n+class Surprise_Algo():\n     def __init__(self, num_recommendations: int = 10):\n         self.reader = Reader(rating_scale=(0, 1))\n         self.model = SVDpp()\n         self.num_recommendations = num_recommendations\n"
                },
                {
                    "date": 1647309577151,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,8 +7,61 @@\n from src.data_processing.data_processing_utils import create_movie_id_mappings_from_file\n import time\n \n \n+def create_movie_id_mappings_from_file(filename: str, create_pickle: bool=True) -> Tuple[Dict[int, str], Dict[str, int]]:\n+    \"\"\"\n+    Given a filename, create a id for each moviename in the file and\n+    generate maps to and from that id.\n+    Preserves the order that the movie ids are given in the file.\n+    If create_pkl is true, then we additionally generate a pickle file\n+    with the given mappings.\n+    \"\"\"\n+    movie_names = []\n+    for _, _, movie_name, _ in read_dataset(filename):\n+        movie_names.append(movie_name)\n+    movie_id_to_name, movie_name_to_id = create_movie_id_mappings(movie_names)\n+    if create_pickle:\n+        rootname, file_extension = os.path.splitext(filename)\n+        with open(rootname + '_movie_id_mappings.pkl', 'wb') as f:\n+            pickle.dump([movie_id_to_name, movie_name_to_id], f)\n+    return movie_id_to_name, movie_name_to_id\n+\n+\n+def create_movie_id_mappings(movie_ids: Iterable[str]) -> Tuple[Dict[int, str], Dict[str, int]]:\n+    \"\"\"\n+    Create mappings from movie names to movie ids and back.\n+    Preserves the order that the movie ids are given in.\n+    \"\"\"\n+    movie_name_to_id : Dict[str, int] = dict()\n+    movie_id_to_name : Dict[int, str] = dict()\n+    # unlike set(), dict is guaranteed to preserve\n+    # insertion order in python3.7+\n+    unique_movie_names = list(dict.fromkeys(movie_ids)) \n+    for id, movie in enumerate(unique_movie_names):\n+        movie_id_to_name[id] = movie\n+        movie_name_to_id[movie] = id\n+    return movie_id_to_name, movie_name_to_id\n+\n+def create_dataset_from_file_with_mapping(data_path: str,\n+        movie_name_to_id: Dict[str, int]) -> List[Tuple[int, int, float]]:\n+    \"\"\"\n+    Transform file contents into a representation\n+    of a dataset (user_id, movie_id, similarity) so as to be\n+    consistent with the given mapping.\n+    If a movie is found whose name is not in the map, it will recieve a movie id of len(movie_name_to_id)\n+    (If needed, we can edit this to increase the size of the movie_name_to_id and\n+    update a new movie_id_to_name parameter,\n+    but I can only see this part of the method used for hidden validation data so there's no point.)\n+    \"\"\"\n+    dataset = []\n+    for _, user_id, movie_id_str, similarity in read_dataset(data_path):\n+        try:\n+            dataset.append((user_id, movie_name_to_id[movie_id_str], similarity))\n+        except KeyError:\n+            dataset.append((user_id, len(movie_name_to_id), similarity))\n+    return dataset\n+\n class Surprise_Algo():\n     def __init__(self, num_recommendations: int = 10):\n         self.reader = Reader(rating_scale=(0, 1))\n         self.model = SVDpp()\n"
                },
                {
                    "date": 1647309583804,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -4,8 +4,9 @@\n import pandas as pd\n import pickle\n from src.data_processing.create_data import create_dataset_from_file_with_mapping\n from src.data_processing.data_processing_utils import create_movie_id_mappings_from_file\n+import os\n import time\n \n \n def create_movie_id_mappings_from_file(filename: str, create_pickle: bool=True) -> Tuple[Dict[int, str], Dict[str, int]]:\n"
                },
                {
                    "date": 1647309589464,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,7 +1,7 @@\n from surprise import SVDpp\n from surprise import Dataset, Reader\n-from typing import List, Tuple, Dict\n+from typing import List, Tuple, Dict, Iterable\n import pandas as pd\n import pickle\n from src.data_processing.create_data import create_dataset_from_file_with_mapping\n from src.data_processing.data_processing_utils import create_movie_id_mappings_from_file\n"
                },
                {
                    "date": 1647309619434,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,14 +2,26 @@\n from surprise import Dataset, Reader\n from typing import List, Tuple, Dict, Iterable\n import pandas as pd\n import pickle\n-from src.data_processing.create_data import create_dataset_from_file_with_mapping\n-from src.data_processing.data_processing_utils import create_movie_id_mappings_from_file\n import os\n import time\n+import datetime\n \n+def read_dataset(data_path: str) -> Iterator[Tuple[datetime, int, str, float]]:\n+    \"\"\"\n+    Given a path to a dataset, return a generator that yields tuples\n+    (time, user_id, movie_id_str, similarity_score) from that dataset.\n+    \"\"\"\n+    with open(data_path, 'r') as f:\n+        for line in f.readlines():\n+            time_str, user_id_str, movie_id_str, similarity_str = line.split(',')\n+            time = datetime.fromisoformat(time_str)\n+            user_id = int(user_id_str)\n+            similarity = float(similarity_str)\n+            yield time, user_id, movie_id_str, similarity\n \n+\n def create_movie_id_mappings_from_file(filename: str, create_pickle: bool=True) -> Tuple[Dict[int, str], Dict[str, int]]:\n     \"\"\"\n     Given a filename, create a id for each moviename in the file and\n     generate maps to and from that id.\n"
                },
                {
                    "date": 1647309627300,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,7 +1,7 @@\n from surprise import SVDpp\n from surprise import Dataset, Reader\n-from typing import List, Tuple, Dict, Iterable\n+from typing import List, Tuple, Dict, Iterable, Iterator\n import pandas as pd\n import pickle\n import os\n import time\n"
                },
                {
                    "date": 1647310007530,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -122,9 +122,9 @@\n         return self.model.predict(uid=user_id, iid=movie_id).est\n \n \n if __name__ == \"__main__\":\n-    train_path = 'data/dataset_partition_1.csv'\n+    train_path = 'data/dataset_partition.csv'\n     movie_id_to_name, movie_name_to_id = create_movie_id_mappings_from_file(train_path)\n     train_data = create_dataset_from_file_with_mapping(\n         train_path, movie_name_to_id)\n     baseline = Surprise_Algo(num_recommendations=20)\n"
                },
                {
                    "date": 1647310078384,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,9 +6,9 @@\n import os\n import time\n import datetime\n \n-def read_dataset(data_path: str) -> Iterator[Tuple[datetime, int, str, float]]:\n+def read_dataset(data_path: str) -> Iterator[Tuple[Datetime, int, str, float]]:\n     \"\"\"\n     Given a path to a dataset, return a generator that yields tuples\n     (time, user_id, movie_id_str, similarity_score) from that dataset.\n     \"\"\"\n"
                },
                {
                    "date": 1647310088294,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -4,11 +4,11 @@\n import pandas as pd\n import pickle\n import os\n import time\n-import datetime\n+from datetime import datetime\n \n-def read_dataset(data_path: str) -> Iterator[Tuple[Datetime, int, str, float]]:\n+def read_dataset(data_path: str) -> Iterator[Tuple[datetime, int, str, float]]:\n     \"\"\"\n     Given a path to a dataset, return a generator that yields tuples\n     (time, user_id, movie_id_str, similarity_score) from that dataset.\n     \"\"\"\n"
                },
                {
                    "date": 1647310099590,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -122,9 +122,9 @@\n         return self.model.predict(uid=user_id, iid=movie_id).est\n \n \n if __name__ == \"__main__\":\n-    train_path = 'data/dataset_partition.csv'\n+    train_path = 'dataset_partition.csv'\n     movie_id_to_name, movie_name_to_id = create_movie_id_mappings_from_file(train_path)\n     train_data = create_dataset_from_file_with_mapping(\n         train_path, movie_name_to_id)\n     baseline = Surprise_Algo(num_recommendations=20)\n"
                },
                {
                    "date": 1647310167916,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -120,8 +120,13 @@\n \n     def get_similarity(self, user_id: int, movie_id: int) -> float:\n         return self.model.predict(uid=user_id, iid=movie_id).est\n \n+def make_model():\n+    movie_id_to_name, movie_name_to_id = create_movie_id_mappings_from_file(train_path)\n+    train_data = create_dataset_from_file_with_mapping(train_path, movie_name_to_id)\n+    baseline = Surprise_Algo(num_recommendations=20)\n+    \n \n if __name__ == \"__main__\":\n     train_path = 'dataset_partition.csv'\n     movie_id_to_name, movie_name_to_id = create_movie_id_mappings_from_file(train_path)\n"
                },
                {
                    "date": 1647310192209,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -124,8 +124,10 @@\n def make_model():\n     movie_id_to_name, movie_name_to_id = create_movie_id_mappings_from_file(train_path)\n     train_data = create_dataset_from_file_with_mapping(train_path, movie_name_to_id)\n     baseline = Surprise_Algo(num_recommendations=20)\n+    baseline.train(train_data)\n+    return baseline\n     \n \n if __name__ == \"__main__\":\n     train_path = 'dataset_partition.csv'\n"
                },
                {
                    "date": 1647310292235,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -125,9 +125,9 @@\n     movie_id_to_name, movie_name_to_id = create_movie_id_mappings_from_file(train_path)\n     train_data = create_dataset_from_file_with_mapping(train_path, movie_name_to_id)\n     baseline = Surprise_Algo(num_recommendations=20)\n     baseline.train(train_data)\n-    return baseline\n+    return baseline, movie_name_to_id\n     \n \n if __name__ == \"__main__\":\n     train_path = 'dataset_partition.csv'\n"
                },
                {
                    "date": 1647311360628,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -121,9 +121,9 @@\n     def get_similarity(self, user_id: int, movie_id: int) -> float:\n         return self.model.predict(uid=user_id, iid=movie_id).est\n \n def make_model():\n-    movie_id_to_name, movie_name_to_id = create_movie_id_mappings_from_file(train_path)\n+    movie_id_to_name, movie_name_to_id = create_movie_id_mappings_from_file('dataset_partition.csv')\n     train_data = create_dataset_from_file_with_mapping(train_path, movie_name_to_id)\n     baseline = Surprise_Algo(num_recommendations=20)\n     baseline.train(train_data)\n     return baseline, movie_name_to_id\n"
                },
                {
                    "date": 1647311396085,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -120,10 +120,10 @@\n \n     def get_similarity(self, user_id: int, movie_id: int) -> float:\n         return self.model.predict(uid=user_id, iid=movie_id).est\n \n-def make_model():\n-    movie_id_to_name, movie_name_to_id = create_movie_id_mappings_from_file('dataset_partition.csv')\n+def make_model(train_path):\n+    movie_id_to_name, movie_name_to_id = create_movie_id_mappings_from_file(train_path)\n     train_data = create_dataset_from_file_with_mapping(train_path, movie_name_to_id)\n     baseline = Surprise_Algo(num_recommendations=20)\n     baseline.train(train_data)\n     return baseline, movie_name_to_id\n"
                },
                {
                    "date": 1647311802309,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -44,10 +44,10 @@\n     \"\"\"\n     Create mappings from movie names to movie ids and back.\n     Preserves the order that the movie ids are given in.\n     \"\"\"\n-    movie_name_to_id : Dict[str, int] = dict()\n-    movie_id_to_name : Dict[int, str] = dict()\n+    movie_name_to_id : Dict[str, int] = defaultdict(-1)\n+    movie_id_to_name : Dict[int, str] = defaultdict(-1)\n     # unlike set(), dict is guaranteed to preserve\n     # insertion order in python3.7+\n     unique_movie_names = list(dict.fromkeys(movie_ids)) \n     for id, movie in enumerate(unique_movie_names):\n"
                },
                {
                    "date": 1647311810005,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -45,9 +45,9 @@\n     Create mappings from movie names to movie ids and back.\n     Preserves the order that the movie ids are given in.\n     \"\"\"\n     movie_name_to_id : Dict[str, int] = defaultdict(-1)\n-    movie_id_to_name : Dict[int, str] = defaultdict(-1)\n+    movie_id_to_name : Dict[int, str] = defaultdict(\"\")\n     # unlike set(), dict is guaranteed to preserve\n     # insertion order in python3.7+\n     unique_movie_names = list(dict.fromkeys(movie_ids)) \n     for id, movie in enumerate(unique_movie_names):\n"
                },
                {
                    "date": 1647311815747,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -44,10 +44,10 @@\n     \"\"\"\n     Create mappings from movie names to movie ids and back.\n     Preserves the order that the movie ids are given in.\n     \"\"\"\n-    movie_name_to_id : Dict[str, int] = defaultdict(-1)\n-    movie_id_to_name : Dict[int, str] = defaultdict(\"\")\n+    movie_name_to_id : Dict[str, int] = dict()\n+    movie_id_to_name : Dict[int, str] = dict()\n     # unlike set(), dict is guaranteed to preserve\n     # insertion order in python3.7+\n     unique_movie_names = list(dict.fromkeys(movie_ids)) \n     for id, movie in enumerate(unique_movie_names):\n"
                }
            ],
            "date": 1647309532880,
            "name": "Commit-0",
            "content": "from surprise import SVDpp\nfrom surprise import Dataset, Reader\nfrom typing import List, Tuple, Dict\nimport pandas as pd\nimport pickle\nfrom constants.model_interface import MovieRecommender\nfrom src.data_processing.create_data import create_dataset_from_file_with_mapping\nfrom src.data_processing.data_processing_utils import create_movie_id_mappings_from_file\nimport time\n\n\nclass Surprise_Algo(MovieRecommender):\n    def __init__(self, num_recommendations: int = 10):\n        self.reader = Reader(rating_scale=(0, 1))\n        self.model = SVDpp()\n        self.num_recommendations = num_recommendations\n\n    def build_features(self, train_data):\n        train_data = pd.DataFrame(train_data, columns=['user_id', 'movie_id', 'rating'])\n        features = Dataset.load_from_df(train_data[['user_id', 'movie_id', 'rating']], self.reader)\n        return features\n\n    def train(self, correspondences: List[Tuple[int, int, float]], save_path=None) -> None:\n        \"\"\"\n        Train the model on the provided correspondences.\n        Each correspondence is of the form\n        user_id, movie_id, similarity_score\n        and it indicates how correlated a user and movie\n        are - higher values mean the user likes the movie\n        more.\n        \"\"\"\n        features = self.build_features(correspondences)\n        self.model.fit(features.build_full_trainset())\n\n        if save_path is not None:\n            with open(save_path, 'wb') as f:\n                pickle.dump(self, f)\n\n    def get_recommendations(self, user_id: int) -> List[int]:\n        \"\"\"\n        Very very inefficient: O(Mlog M) where M is the number of known movie IDs,\n        the constant factor is big too because of self.model.predict being called in\n        a for loop (probablly a matrix multiplication inside)\n        \"\"\"\n        all_known_movies = self.model.trainset._raw2inner_id_items.keys()\n        my_recs = []\n        for movie_id in all_known_movies:\n            my_recs.append((movie_id, self.model.predict(uid=user_id, iid=movie_id).est))\n\n        preds_df = pd.DataFrame(my_recs, columns=['movie_id', 'predictions']).\\\n            sort_values('predictions', ascending=False).head(self.num_recommendations)\n\n        recommendation_ids = preds_df['movie_id'].tolist()\n        return recommendation_ids\n\n    def get_similarity(self, user_id: int, movie_id: int) -> float:\n        return self.model.predict(uid=user_id, iid=movie_id).est\n\n\nif __name__ == \"__main__\":\n    train_path = 'data/dataset_partition_1.csv'\n    movie_id_to_name, movie_name_to_id = create_movie_id_mappings_from_file(train_path)\n    train_data = create_dataset_from_file_with_mapping(\n        train_path, movie_name_to_id)\n    baseline = Surprise_Algo(num_recommendations=20)\n    start_train_time = time.perf_counter()\n    baseline.train(train_data)\n    train_time = time.perf_counter() - start_train_time\n    print(\"model train time: {}\".format(train_time))\n    recommendation_ids = baseline.get_recommendations(123)\n    recommendation_ids = baseline.get_recommendations(9999999999)\n    print(recommendation_ids)\n    movies = []\n    for item in recommendation_ids:\n        movies.append(movie_id_to_name[item])\n    print(movies)\n    print(baseline.get_similarity(567781,129))"
        }
    ]
}